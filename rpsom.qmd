---
title: "RPS.bot: Opponent Modeling"
bread-crumbs: false
format:
  html:
    math: true
---
At the heart of opponent modeling is predicting your opponent's next move. While ![game theory optimal](intro.qmd#expected-value) play is an interesting mathematical exercise, it's often not actually the most rewarding strategy in practice. 

Humans tend to be predictably irrational. This can manifest itself as someone having favorite preferred moves, making "tilted" decisions, reacting to previous games in predictable ways, or having "leaks" like never throwing the same move three times in a row. 

There can be levels to the game -- maybe you think your opponent expects you to play Rock, so then you expect that they'll play Paper, so you should play Scissors, but maybe they'll realize this and play Rock to counter the Scissors, so you should actually play Paper, and so on. 

We use suboptimal bots in our challenge games to reinforce thinking strategically about how to respond to opponents. The overall goal is to maximize payoffs, which is in practice taking advantage of opponent weaknesses while minimizing your own exploitability. 

## Axes of opponent modeling
There are two main axes for opponent modeling: opponents and games. 

### Types of opponents 
**Bad static opponents:** Playing Rock 

**Better static opponents:** Playing 60% Rock, 20% Paper, 20% Scissors

**Dynamic with no relation to game, i.e. a static Markov model:** Playing RPSRPSRPS...

![](markovrps.png)

**Dynamic only related to limited subset of actions, like:** 

1. Only our own actions: Randomly play an action that we didn't play in the last 2 games
2. Only opponent actions: Play the action that beats the most frequent of the opponent's last 3 moves
3. Only most recent action: Copy the opponent's last move

**Simple dynamic related to entire history of actions:** Play the move that beats the most frequent opponent move so far

**Advanced dynamic relatd to entire history of actions:** Sophisticated adversarial prediction algorithm or set of algorithms

### Types of games/knowledge
Play a single known opponent (known meaning you know their entire strategy)

Play multiple known opponents

Play a single unknown opponent

Play multiple unknown opponents

[Play multiple unknown opponents that include some suboptimal house bots and some human-created bots](rpsbotchallenge.qmd)

## Starter Code
We provide Python starter code that you can use to build RPS bots.  

The code by default plays randomly, which will [guarantee a breakeven result](intro.qmd#nash-equilibrium-strategy-for-rps) (and won't be very fun). 

The main actions are in the `get_action` function: 

```python
def get_action(self, *, match_clock):
  return random.choice([RockAction(), PaperAction(), ScissorsAction()])
```

### Variables
The profit so far in the match is stored in: 

- `self.my_profit`

And the history of your actions and your opponent actions are stored in the following array, where each history is a tuple of `(my_action, their_action)`.  

- `self.history[]`

### Functions
- `def __init__(self)`: 
  - Initializes profit `self.my_profit`
  - Initializes the history array `self.history[]`
- `handle_results(self, *, my_action, their_action, my_payoff, match_clock)`: 
  - This is run at the end of every game
  - It updates the history with `my_action` and `their_action` 
  - It updates the payoff with `my_payoff`
  - `match_clock` is used to see how much of your 30 second total time is remaining (this can be ignored) 
- `def get_action(self, *, match_clock)`: 
  - This is the main function where you run your strategy and return an action
  - You can return `RockAction()`, `PaperAction()`, or `ScissorsAction()`

## Beating simple known opponents
It's a lot easier to beat your opponents when you know their strategy! This is still a worthwhile first exercise for thinking about which strategies beat which opponent strategies.

### Rockbot
Opponent strategy: 100% Rock

Counter-strategy: 100% Paper

EV: +1 per game

```python
def get_action(self, *, match_clock):
  return PaperAction()
```

### r532bot
Opponent strategy: 50% Rock, 30% Paper, 20% Scissors

Counter-strategy: 100% Paper

EV: $0.5(1) + 0.3(0) + 0.2(-1) = 0.3$ per game

```python
def get_action(self, *, match_clock):
  return PaperAction()
```

### RPSbot
Opponent strategy: RPSRPSRPS...

Counter-strategy: PSRPSRPSR...

EV: +1 per game

```python
def get_action(self, *, match_clock):
    moves = [PaperAction(), ScissorsAction(), RockAction()]
    move_index = len(self.history) % 3  # Use history length to cycle
    return moves[move_index]
```

### Combination of the first 3 bots

### Mimicbot
Opponent strategy: RPSRPSRPS...

Counter-strategy: PSRPSRPSR...

EV: +1 per game

### Beatprevbot
This bot plays to beat the opponent's last action. 

```python
def get_action(self, *, match_clock):
  if self.history: # After the 1st game
      last_opponent_move = self.history[-1][1] # Find last opponent move
      if isinstance(last_opponent_move, RockAction): # If Rock
          return PaperAction() # Play Paper
      elif isinstance(last_opponent_move, PaperAction): # If Paper
          return ScissorsAction() # Play Scissors
      else: # If Scissors
          return RockAction() # Play Rock
  else: # 1st game play randomly 
      return random.choice([RockAction(), PaperAction(), ScissorsAction()])
```

Opponent strategy: RPSRPSRPS...

Counter-strategy: PSRPSRPSR...

EV: +1 per game

This bot plays to beat the opponent playing the above "win vs. last action" strategy.  

If our last action is Rock, they would play Paper, so we should play Scissors. 

If our last action is Paper, they would play Scissors, so we should play Rock. 

If our last action is Scissors, they would play Rock, so we should play Paper. 

```python
def get_action(self, *, match_clock):
  if self.history: # After the 1st game
      our_last_move = self.history[-1][0] # Find our last move
      if isinstance(our_last_move, RockAction): # If Rock
          return ScissorsAction() # Play Scissors
      elif isinstance(our_last_move, PaperAction): # If Paper
          return RockAction() # Play Rock
      else: # If Scissors
          return PaperAction() # Play Paper
  else: # 1st game play randomly 
      return random.choice([RockAction(), PaperAction(), ScissorsAction()])
```

### Freqbot 
Opponent strategy: RPSRPSRPS...

Counter-strategy: PSRPSRPSR...

EV: +1 per game

Instead of just looking at the last action, this bot plays to beat the most frequent opponent action over the entire history. 

```python
def get_action(self, *, match_clock):
  if self.history: # After the 1st game
      opponent_moves = [move[1] for move in self.history]
      
      # Count the occurrences of each move
      move_counts = {} # Count each R/P/S
      for move in opponent_moves:
          if move in move_counts:
              move_counts[move] += 1 # Add to counter
          else:
              move_counts[move] = 1 # Start counter
      
      # Find the move with the highest count
      most_common_move = None
      highest_count = 0
      for move, count in move_counts.items():
          if count > highest_count:
              most_common_move = move
              highest_count = count
      
      if isinstance(most_common_move, RockAction):
          return PaperAction()
      elif isinstance(most_common_move, PaperAction):
          return ScissorsAction()
      else: # most_common_move is ScissorsAction
          return RockAction()
  else: # 1st game play randomly 
      return random.choice([RockAction(), PaperAction(), ScissorsAction()])
```

### Combination of all 6 bots		

## General strategies
In poker, many players start with GTO then exploit

don't know the opponent in advance? can try to detect exact, detect deviation from GTO (direlecht?), or predictors

go 1 more level on one of these 

A few possible strategies are: 

- Single out specific bots that are exploitable and write code to identify and exploit them
  - Build an ensemble of exploits targeting many opponents
- Attempt to predict the next move of each bot using a general algorithm
- Use random play as a backup plan (see [below](#game-theory-equilibrium) for a full explanation of why this Nash equilibrium strategy always breaks even)
- Simple strategies to do decently 

### Win vs. Last Action


### Win vs. Opponent Playing "Win vs. Last Action"


### Sometimes Playing Randomly 
Since there are many suboptimal environment bots, breaking even by playing randomly won't be a good enough overall strategy, but could be valuable to consider in some situations. Here is an example of using random play in combination with another strategy. 

One of the bots above is [Win vs. Last Action](#win-vs.-last-action). Suppose that you have a theory that you should actually play to beat the action that they played *two* games ago. 

You could try something like this: 

- If you're winning or tying, then play to beat the action from two games ago 80% of the time and play randomly 20% of the time so that you aren't too predictable
- If you're losing (i.e. maybe this strategy is not working well), then always play randomly

```python
def get_action(self, *, match_clock):

  # If losing, play randomly
  if self.my_profit < 0:
      return random.choice([RockAction(), PaperAction(), ScissorsAction()])

  # If winning or tying, and there are at least 2 rounds of history
  if len(self.history) >= 2:
      # 80% chance to play based on opponent's action from 2 rounds ago
      if random.random() < 0.8:
          two_rounds_ago = self.history[-2][1]  # Opponent's action from 2 rounds ago
          if isinstance(two_rounds_ago, RockAction):
              return PaperAction()
          elif isinstance(two_rounds_ago, PaperAction):
              return ScissorsAction()
          else:  # ScissorsAction
              return RockAction()

  # In all other cases (including 1st 2 rounds), play randomly
  return random.choice([RockAction(), PaperAction(), ScissorsAction()])
```

### Exploiting a Weak Bot
In practice in this hackathon, a lot of value will come from exploiting weak bots or other opponents. 

For example, you could check to see if any bot is always playing Rock. If so, you always play Paper. If not, you play randomly. Then you will breakeven against everyone, but crush the Rock-only bot. 

Here's how to execute this strategy: 

```python
def get_action(self, *, match_clock):
  if self.history: # Check if the opponent has played at least once
      # Get all of the opponent's moves
      opponent_moves = [move[1] for move in self.history]
      
      # Count how many times the opponent played Rock
      rock_count = sum(1 for move in opponent_moves if isinstance(move, RockAction))
      
      # If opponent has only played Rock, we play Paper
      if rock_count == len(opponent_moves):
          return PaperAction()

  # If it's the first move or opponent hasn't only played Rock, play randomly
  return random.choice([RockAction(), PaperAction(), ScissorsAction()])
```

### Claudebot

## Predicting
How to parametrize opponent
Multiple hypotheses, run counterfactually, pick best one  
Pattern hypotheses, compare to general pattern about something else

Beat the thing opponent played after last hand = the prev hand
most recent same history 

### Iocaine Powder

## Challenges
Build a single program that levels up as we give you progressively more difficult training games → enter into the weekly challenge by the end (or skip directly to that)