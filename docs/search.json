[
  {
    "objectID": "botsubmit.html",
    "href": "botsubmit.html",
    "title": "RPS Hackathon @ Recurse: Submit",
    "section": "",
    "text": "Edit Code\n\n\nUpload File\n\n\nLink Repo\n\n\n\n\nSubmit\n\n\n\n\n\n\n\n\n\n\n\nUpload File\n\n\n\n\nThe file you upload should be a Python file that looks something like this. You can download this file in the rps-engine repo at players/default/player.py and for more info on testing see: Game Engine.\n# Simple example bot, written in Python.\n\nfrom skeleton.actions import RockAction, PaperAction, ScissorsAction\nfrom skeleton.bot import Bot\nfrom skeleton.runner import parse_args, run_bot\n\nimport random\n\nclass Player(Bot):\n    # A bot for playing Rock-Paper-Scissors.\n\n    def __init__(self):\n        # Called when a new matchup starts. Called exactly once.\n        \n        self.my_profit = 0\n        self.history = []\n\n    def handle_results(self, *, my_action, their_action, my_payoff, match_clock):\n        # Called after a round. Called NUM_ROUNDS times.\n        \n        self.history.append((my_action, their_action))\n        self.my_profit += my_payoff\n\n    def get_action(self, *, match_clock):\n        # Where the magic happens. Called when the engine needs an action from\n        # your bot. Called NUM_ROUNDS times.\n        #\n        # Returns a RockAction(), PaperAction(), or ScissorsAction().\n        \n        return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\nif __name__ == '__main__':\n    run_bot(Player(), parse_args())\n\n\n\nComing soon! Ask if you’d like to use this to upload multiple files or something else that doesn’t fit into a player.py file."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "RPS.bot: About",
    "section": "",
    "text": "RPS.bot by Max Chiswick and Ross Rheingans-Yoo.\nWe’re excited about learning through games. Check out Overbet.ai (under development) and Poker Camp for more."
  },
  {
    "objectID": "about.html#about-us",
    "href": "about.html#about-us",
    "title": "RPS.bot: About",
    "section": "",
    "text": "RPS.bot by Max Chiswick and Ross Rheingans-Yoo.\nWe’re excited about learning through games. Check out Overbet.ai (under development) and Poker Camp for more."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RPS.bot",
    "section": "",
    "text": "RPS.bot is an intro to opponent modeling through the game Rock Paper Scissors.\nGet started now with Intro to RPS.",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "RPS.bot: Intro",
    "section": "",
    "text": "Image by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#rules",
    "href": "intro.html#rules",
    "title": "RPS.bot: Intro",
    "section": "",
    "text": "Image by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#why",
    "href": "intro.html#why",
    "title": "RPS.bot: Intro",
    "section": "Why?",
    "text": "Why?\nWe think RPS is fun and while it’s simple enough that everyone knows it and understands the rules, it’s still a great domain for thinking about strategy in a repeated game against a variety of opponents.\nFrom DeepMind in 2023:\n\nIn sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.\n\n\n\ngonna 3d print the exploitability of a player’s rock-paper-scissors strategy over their strategy simplex pic.twitter.com/cSXFKyI5E8\n\n— Kevin a. Wang (@often_wang) September 24, 2024",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#rps-game-theory",
    "href": "intro.html#rps-game-theory",
    "title": "RPS.bot: Intro",
    "section": "RPS game theory",
    "text": "RPS game theory\n\nPayoff matrix\nThe core features of a game are its players, the actions of each player, and the payoffs. We can show this for RPS in the below payoff matrix, also known as normal-form.\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe payoffs for Player 1 are on the left and for Player 2 on the right in each payoff outcome of the game. For example, the bottom left payoff is when Player 1 plays Scissors and Player 2 plays Rock, resulting in -1 for P1 and +1 for P2.\nA strategy says which actions you would take for every state of the game.\n\n\nExpected value\nExpected value in a game represents the average outcome of a strategy if it were repeated many times. It’s calculated by multiplying each possible outcome by its probability of occurrence and then summing these products.\nSuppose that Player 1 plays the strategy:\n\n\\begin{cases}\nr_1 = 0.5 \\\\\np_1 = 0.25 \\\\\ns_1 = 0.25\n\\end{cases}\n\nand Player 2 plays the strategy:\n\n\\begin{cases}\nr_2 = 0.1 \\\\\np_2 = 0.3 \\\\\ns_2 = 0.6\n\\end{cases}\n\nLet’s add these to the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p_1=0.25)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s_1=0.25)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nTo simplify, let’s just write the payoffs for Player 1 since payoffs for Player 2 will simply be the opposite:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n0\n-1\n1\n\n\nPaper (p_1=0.25)\n1\n0\n-1\n\n\nScissors (s_1=0.25)\n-1\n1\n0\n\n\n\nNow we can multiply the player action strategies together to get a percentage occurrence for each payoff in the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\nVal: 0 Pr: 0.5(0.1) = .05\nVal: -1 Pr: 0.5(0.3) = .15\nVal: 1 Pr: 0.5(0.6) = .3\n\n\nPaper (p_1=0.25)\nVal: 1 Pr: 0.25(0.1) = .025\nVal: 0 Pr: 0.25(0.3) = .075\nVal: -1 Pr: 0.25(0.6) = .15\n\n\nScissors (s_1=0.25)\nVal: -1 Pr: 0.25(0.1) = .025\nVal: 1 Pr: 0.25(0.3) = .075\nVal: 0 Pr: 0.25(0.6) = .15\n\n\n\nNote that the total probabilities sum to 1 and each row and column sums to the probability of playing that row or column.\nWe can work out the expected value of the game to Player 1 (summing all payoffs multiplied by probabilities from top left to bottom right):\n\\mathbb{E}[P_1] = 0(0.05) + -1(0.15) + 1(0.3) + 1(0.025) + 0(0.075) + -1(0.15) + -1(0.025) + 1(0.075) + 0(0.15) = 0.075\nTherefore P1 is expected to gain 0.075 per game given these strategies. Since payoffs are reversed for P2, P2’s expectation is -0.075 per game.\n\n\nZero-sum\nWe see in the matrix that every payoff is zero-sum, i.e. the sum of the payoffs to both players is 0. This means the game is one of pure competition. Any amount P1 wins is from P2 and vice versa.\n\n\nNash equilibrium\nA Nash equilibrium means that no player can improve their expected payoff by unilaterally changing their strategy. That is, changing one’s strategy can only result in the same or worse payoff (assuming the other player does not change).\nIn RPS, the Nash equilibrium strategy is to play each action r = p = s = 1/3 of the time. I.e., to play totally randomly.\nPlaying a combination of strategies is called a mixed strategy, as opposed to a pure strategy, which would select only one action. Mixed strategies are useful in games of imperfect information because it’s valuable to not be predictable and to conceal your private information. In perfect information games, the theoretically optimal play would not contain any mixing (i.e., if you could calculate all possible moves to the end of the game).\nThe equilibrium RPS strategy is worked out below:\n\n\n\n\n\n\nNash equilibrium strategy for RPS\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock (r)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nIf Player 1 plays Rock with probability r, Paper with probability p, and Scissors with probability s, we have the following expected value equations for Player 2:\n\\mathbb{E}[P_2(R)] = 0r -1p + 1s\n\\mathbb{E}[P_2(P)] = 1r + 0p - 1s\n\\mathbb{E}[P_2(S)] = -1r + 1p + 0s\n(To compute each of these, sum the payoffs for each column with P2 payoffs and P1 probabilities. P2 payoffs because we are calculating the expected payoffs for P2 and P1 probabilities because the payoffs depend on the strategy probabilties of P1 against each of P2’s actions.)\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for r, p, and s, we can start by setting these EVs equal:\n\\mathbb{E}[P_2(R)] = \\mathbb{E}[P_2(P)]\n-1p + 1s = 1r - 1s\n2s = p + r\nThen setting these equal:\n\\mathbb{E}[P_2(R)] = \\mathbb{E}[P_2(S)]\n-1p + 1s = -1r + 1p\ns + r = 2p\nAnd finally setting these equal:\n\\mathbb{E}[P_2(P)] = \\mathbb{E}[P_2(S)]\n1r - 1s = -1r + 1p\n2r = s + p\nNow we have these equations:\n\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\nWe can rewrite the 1st:\nr = 2s - p\nAnd combine with the 2nd:\ns + (2s - p) = 2p\n3s = 3p\nResulting in:\ns = p\nNow we can go back to the 2nd equation:\ns + r = 2p\nAnd insert s = p:\ns + r = 2s\nAnd arrive at:\nr = s\nWe now see that all are equal:\ns = p = r\nWe also know that they must all sum to 1:\nr + p + s = 1\nSince they’re all equal and sum to 1, we can substitute p and s with r:\n3r = 1\nr = 1/3\nSo all actions are taken with probability 1/3:\nr = p = s = 1/3 \\quad \\blacksquare\nBy symmetry, the same equilibrium strategy is true for Player 2.\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= 0(r) + 1(p) + -1(s) \\\\\n&= 0(1/3) + 1(1/3) + -1(1/3) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nHow about the case of the opponent playing 60% Rock, 20% Paper, 20% Scissors?\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. 622}) &= 0.6(\\text{Equilibrium vs. Rock}) \\\\\n&\\quad{}+ 0.2(\\text{Equilibrium vs. Paper}) \\\\  \n&\\quad{}+ 0.2(\\text{Equilibrium vs. Scissors}) \\\\\n&= 0.6(0) + 0.2(0) + 0.2(0) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nThe random equilibrium strategy will result in 0 against any pure strategy and any combination of strategies including 622 and the opponent playing the random strategy.\n\n\nExploiting vs. Nash\nThe equilibrium strategy vs. a pure Rock opponent is a useful illustration of the limitations of playing at equilibrium. The Rock opponent is playing perhaps the worst possible strategy, yet equilibrium is still breaking even against it!\nWhat’s the best that we could do against Rock only? We could play purely paper. This is called a best response strategy. The payoffs are written for playing Paper and the probabilities indicate the opponent playing only Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. Rock}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(1) + 0(0) + -1(0) \\\\\n&= 1\n\\end{split}\n\\end{equation}\n\nWe’d win 1 each game playing Paper vs. Rock. Though now always playing Paper means that the opponent could always play Scissors and then we’d become the loser.\nHow about against the opponent playing 60% Rock, 20% Paper, 20% Scissors? Here we can see that because they are overplaying Rock, our best strategy is again to always play Paper. We write the payoffs for playing Paper and the probabilities according to the 622 strategy.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. 622}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(0.6) + 0(0.2) + -1(0.2) \\\\\n&= 0.6 + 0 - 0.2 \\\\\n&= 0.4\n\\end{split}\n\\end{equation}\n\nPlaying Paper vs. 622 results in an expected win of 0.4 per game.",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "rpsom.html",
    "href": "rpsom.html",
    "title": "RPS.bot: Opponent Modeling",
    "section": "",
    "text": "At the heart of opponent modeling is predicting your opponent’s next move. While  play is an interesting mathematical exercise, it’s often not actually the most rewarding strategy in practice.\nHumans tend to be predictably irrational. This can manifest itself as someone having favorite preferred moves, making “tilted” decisions, reacting to previous games in predictable ways, or having “leaks” like never throwing the same move three times in a row.\nThere can be levels to the game – maybe you think your opponent expects you to play Rock, so then you expect that they’ll play Paper, so you should play Scissors, but maybe they’ll realize this and play Rock to counter the Scissors, so you should actually play Paper, and so on.\nWe use suboptimal bots in our challenge games to reinforce thinking strategically about how to respond to opponents. The overall goal is to maximize payoffs, which is in practice taking advantage of opponent weaknesses while minimizing your own exploitability.",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#axes-of-opponent-modeling",
    "href": "rpsom.html#axes-of-opponent-modeling",
    "title": "RPS.bot: Opponent Modeling",
    "section": "Axes of opponent modeling",
    "text": "Axes of opponent modeling\nThere are two main axes for opponent modeling: opponents and games.\n\nTypes of opponents\nBad static opponents: Playing Rock\nBetter static opponents: Playing 60% Rock, 20% Paper, 20% Scissors\nDynamic with no relation to game, i.e. a static Markov model: Playing RPSRPSRPS…\n\nDynamic only related to limited subset of actions, like:\n\nOnly our own actions: Randomly play an action that we didn’t play in the last 2 games\nOnly opponent actions: Play the action that beats the most frequent of the opponent’s last 3 moves\nOnly most recent action: Copy the opponent’s last move\n\nSimple dynamic related to entire history of actions: Play the move that beats the most frequent opponent move so far\nAdvanced dynamic relatd to entire history of actions: Sophisticated adversarial prediction algorithm or set of algorithms\n\n\nTypes of games/knowledge\nPlay a single known opponent (known meaning you know their entire strategy)\nPlay multiple known opponents\nPlay a single unknown opponent\nPlay multiple unknown opponents\nPlay multiple unknown opponents that include some suboptimal house bots and some human-created bots",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#starter-code",
    "href": "rpsom.html#starter-code",
    "title": "RPS.bot: Opponent Modeling",
    "section": "Starter Code",
    "text": "Starter Code\nWe provide Python starter code that you can use to build RPS bots.\nThe code by default plays randomly, which will guarantee a breakeven result (and won’t be very fun).\nThe main actions are in the get_action function:\ndef get_action(self, *, match_clock):\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\nVariables\nThe profit so far in the match is stored in:\n\nself.my_profit\n\nAnd the history of your actions and your opponent actions are stored in the following array, where each history is a tuple of (my_action, their_action).\n\nself.history[]\n\n\n\nFunctions\n\ndef __init__(self):\n\nInitializes profit self.my_profit\nInitializes the history array self.history[]\n\nhandle_results(self, *, my_action, their_action, my_payoff, match_clock):\n\nThis is run at the end of every game\nIt updates the history with my_action and their_action\nIt updates the payoff with my_payoff\nmatch_clock is used to see how much of your 30 second total time is remaining (this can be ignored)\n\ndef get_action(self, *, match_clock):\n\nThis is the main function where you run your strategy and return an action\nYou can return RockAction(), PaperAction(), or ScissorsAction()",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#beating-simple-known-opponents",
    "href": "rpsom.html#beating-simple-known-opponents",
    "title": "RPS.bot: Opponent Modeling",
    "section": "Beating simple known opponents",
    "text": "Beating simple known opponents\nIt’s a lot easier to beat your opponents when you know their strategy! This is still a worthwhile first exercise for thinking about which strategies beat which opponent strategies.\n\nRockbot\nOpponent strategy: 100% Rock\nCounter-strategy: 100% Paper\nEV: +1 per game\ndef get_action(self, *, match_clock):\n  return PaperAction()\n\n\nr532bot\nOpponent strategy: 50% Rock, 30% Paper, 20% Scissors\nCounter-strategy: 100% Paper\nEV: 0.5(1) + 0.3(0) + 0.2(-1) = 0.3 per game\ndef get_action(self, *, match_clock):\n  return PaperAction()\n\n\nRPSbot\nOpponent strategy: RPSRPSRPS…\nCounter-strategy: PSRPSRPSR…\nEV: +1 per game\ndef get_action(self, *, match_clock):\n    moves = [PaperAction(), ScissorsAction(), RockAction()]\n    move_index = len(self.history) % 3  # Use history length to cycle\n    return moves[move_index]\n\n\nCombination of the first 3 bots\n\n\nMimicbot\nOpponent strategy: RPSRPSRPS…\nCounter-strategy: PSRPSRPSR…\nEV: +1 per game\n\n\nBeatprevbot\nThis bot plays to beat the opponent’s last action.\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      last_opponent_move = self.history[-1][1] # Find last opponent move\n      if isinstance(last_opponent_move, RockAction): # If Rock\n          return PaperAction() # Play Paper\n      elif isinstance(last_opponent_move, PaperAction): # If Paper\n          return ScissorsAction() # Play Scissors\n      else: # If Scissors\n          return RockAction() # Play Rock\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\nOpponent strategy: RPSRPSRPS…\nCounter-strategy: PSRPSRPSR…\nEV: +1 per game\nThis bot plays to beat the opponent playing the above “win vs. last action” strategy.\nIf our last action is Rock, they would play Paper, so we should play Scissors.\nIf our last action is Paper, they would play Scissors, so we should play Rock.\nIf our last action is Scissors, they would play Rock, so we should play Paper.\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      our_last_move = self.history[-1][0] # Find our last move\n      if isinstance(our_last_move, RockAction): # If Rock\n          return ScissorsAction() # Play Scissors\n      elif isinstance(our_last_move, PaperAction): # If Paper\n          return RockAction() # Play Rock\n      else: # If Scissors\n          return PaperAction() # Play Paper\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nFreqbot\nOpponent strategy: RPSRPSRPS…\nCounter-strategy: PSRPSRPSR…\nEV: +1 per game\nInstead of just looking at the last action, this bot plays to beat the most frequent opponent action over the entire history.\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      opponent_moves = [move[1] for move in self.history]\n      \n      # Count the occurrences of each move\n      move_counts = {} # Count each R/P/S\n      for move in opponent_moves:\n          if move in move_counts:\n              move_counts[move] += 1 # Add to counter\n          else:\n              move_counts[move] = 1 # Start counter\n      \n      # Find the move with the highest count\n      most_common_move = None\n      highest_count = 0\n      for move, count in move_counts.items():\n          if count &gt; highest_count:\n              most_common_move = move\n              highest_count = count\n      \n      if isinstance(most_common_move, RockAction):\n          return PaperAction()\n      elif isinstance(most_common_move, PaperAction):\n          return ScissorsAction()\n      else: # most_common_move is ScissorsAction\n          return RockAction()\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nCombination of all 6 bots",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#general-strategies",
    "href": "rpsom.html#general-strategies",
    "title": "RPS.bot: Opponent Modeling",
    "section": "General strategies",
    "text": "General strategies\nIn poker, many players start with GTO then exploit\ndon’t know the opponent in advance? can try to detect exact, detect deviation from GTO (direlecht?), or predictors\ngo 1 more level on one of these\nA few possible strategies are:\n\nSingle out specific bots that are exploitable and write code to identify and exploit them\n\nBuild an ensemble of exploits targeting many opponents\n\nAttempt to predict the next move of each bot using a general algorithm\nUse random play as a backup plan (see below for a full explanation of why this Nash equilibrium strategy always breaks even)\nSimple strategies to do decently\n\n\nWin vs. Last Action\n\n\nWin vs. Opponent Playing “Win vs. Last Action”\n\n\nSometimes Playing Randomly\nSince there are many suboptimal environment bots, breaking even by playing randomly won’t be a good enough overall strategy, but could be valuable to consider in some situations. Here is an example of using random play in combination with another strategy.\nOne of the bots above is Win vs. Last Action. Suppose that you have a theory that you should actually play to beat the action that they played two games ago.\nYou could try something like this:\n\nIf you’re winning or tying, then play to beat the action from two games ago 80% of the time and play randomly 20% of the time so that you aren’t too predictable\nIf you’re losing (i.e. maybe this strategy is not working well), then always play randomly\n\ndef get_action(self, *, match_clock):\n\n  # If losing, play randomly\n  if self.my_profit &lt; 0:\n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n  # If winning or tying, and there are at least 2 rounds of history\n  if len(self.history) &gt;= 2:\n      # 80% chance to play based on opponent's action from 2 rounds ago\n      if random.random() &lt; 0.8:\n          two_rounds_ago = self.history[-2][1]  # Opponent's action from 2 rounds ago\n          if isinstance(two_rounds_ago, RockAction):\n              return PaperAction()\n          elif isinstance(two_rounds_ago, PaperAction):\n              return ScissorsAction()\n          else:  # ScissorsAction\n              return RockAction()\n\n  # In all other cases (including 1st 2 rounds), play randomly\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nExploiting a Weak Bot\nIn practice in this hackathon, a lot of value will come from exploiting weak bots or other opponents.\nFor example, you could check to see if any bot is always playing Rock. If so, you always play Paper. If not, you play randomly. Then you will breakeven against everyone, but crush the Rock-only bot.\nHere’s how to execute this strategy:\ndef get_action(self, *, match_clock):\n  if self.history: # Check if the opponent has played at least once\n      # Get all of the opponent's moves\n      opponent_moves = [move[1] for move in self.history]\n      \n      # Count how many times the opponent played Rock\n      rock_count = sum(1 for move in opponent_moves if isinstance(move, RockAction))\n      \n      # If opponent has only played Rock, we play Paper\n      if rock_count == len(opponent_moves):\n          return PaperAction()\n\n  # If it's the first move or opponent hasn't only played Rock, play randomly\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nClaudebot",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#predicting",
    "href": "rpsom.html#predicting",
    "title": "RPS.bot: Opponent Modeling",
    "section": "Predicting",
    "text": "Predicting\nHow to parametrize opponent Multiple hypotheses, run counterfactually, pick best one\nPattern hypotheses, compare to general pattern about something else\nBeat the thing opponent played after last hand = the prev hand most recent same history\n\nIocaine Powder",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#challenges",
    "href": "rpsom.html#challenges",
    "title": "RPS.bot: Opponent Modeling",
    "section": "Challenges",
    "text": "Challenges\nBuild a single program that levels up as we give you progressively more difficult training games → enter into the weekly challenge by the end (or skip directly to that)",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "RPS Hackathon @ Recurse: Tests and Logs",
    "section": "",
    "text": "Latest submission versus last scoring round\n\n\nLoading…\n\n\n\n\n\nNext round\n\nLoading…\n\nMatch logs\n\n\nLoading…"
  },
  {
    "objectID": "leaderboard.html",
    "href": "leaderboard.html",
    "title": "RPS Hackathon @ Recurse: Leaderboard",
    "section": "",
    "text": "Latest freeplay round\n\n\nLoading…\n\n\n\n\n\nLatest scoring round\n\n\nLoading…\n\n\n\n\n\nNext round\n\n\n\nAll scoring rounds\n\n\nLoading…"
  },
  {
    "objectID": "rpsbotchallenge.html",
    "href": "rpsbotchallenge.html",
    "title": "RPS.bot: Bot Challenge",
    "section": "",
    "text": "RPS bot challenges are coming soon!\nIn these challenges, there are always suboptimal “house bots”. Think bots that always play Rock or always play the sequence “RPSRPSRPS…”.\nThis means that playing the game theory optimal strategy will not maximize your payoff against them.\nThe challenge is to predict what they’re doing, and in cases where there are house bots and human-made bots, the challenge is to play well against the house bots without getting counterplayed by the human-made ones. And perhaps to counterplay the human-made bots that are prioritizing counterplaying the house bots!",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#weekly-challenge",
    "href": "rpsbotchallenge.html#weekly-challenge",
    "title": "RPS.bot: Bot Challenge",
    "section": "Weekly Challenge",
    "text": "Weekly Challenge\nEnter your best bot to play against that week’s other human-made bot entrants as well as a selection of our own house bots that will vary each week. You’ll play every other entrant for 1000 games.\nEntries start each Monday and end each Sunday, with results posted the Monday after. [Maybe: Some feedback mechanism to improve and resubmit throughout week, at least against house bots?]\n\nEach week has a special rookie division for those who are entering for their first time.",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#daily-challenge",
    "href": "rpsbotchallenge.html#daily-challenge",
    "title": "RPS.bot: Bot Challenge",
    "section": "Daily Challenge",
    "text": "Daily Challenge\nPlay 200 [how many?] games against a single RPS bot each day.\nThe bots will get roughly more difficult as the week goes on from Monday to Sunday. They’ll be all different types – our own house bots, human-made bots from our weekly challenges, LLM-generated, etc.\nYou can play against the bot with your own bot, manually, or both. We have tools to help write your own bot even if you don’t know how to code.\n[What about the timer play every second thing?]",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#hackathon",
    "href": "rpsbotchallenge.html#hackathon",
    "title": "RPS.bot: Bot Challenge",
    "section": "Hackathon",
    "text": "Hackathon\nWe periodically run virtual and in-person hackathons that we’ll announce on RPS.bot and on our mailing list/social channels.\nThe hackathons are run over a few hours with new rounds every 30 minutes. The format is similar to the weekly challenge above, where you face off against other human-made bots and our own house bots. We enter a new selection of progressively smarter house bots every round. Once a house bot enters, it stays in for subsequent rounds.\nAfter each round, you can see the entire play history against each bot and a matrix score report.\n\nYou can submit multiple times during a round, and for each submission, you’ll get a test result showing how your bot would have performed in the previous round. Doing better in the previous round is a sign of progress, but may not actually result in a better score in the current round, which will have updated participant bots and new house bots.",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  }
]