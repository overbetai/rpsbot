[
  {
    "objectID": "rpstheory.html",
    "href": "rpstheory.html",
    "title": "RPS Hackathon @ Recurse: Guide to Building a Bot",
    "section": "",
    "text": "start with GTO then exploit\nRPS is a zero-sum game and the payouts are symmetrical as follows:\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe Nash Equilibrium strategy is to play each action r = p = s = 1/3 of the time.\n\n\n\n\n\n\nNash Equilibrium Strategy for RPS\n\n\n\n\n\nIf Player 1 plays Rock with probability r, Paper with probability p, and Scissors with probability s, we have the following expected value equations for Player 2:\n\\mathbb{E}(\\text{R}) = -1p + 1s\n\\mathbb{E}(\\text{P}) = 1r - 1s\n\\mathbb{E}(\\text{S}) = -1r + 1p\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for r, p, and s, we can start by setting these EVs equal:\n\\mathbb{E}(\\text{R}) = \\mathbb{E}(\\text{P})\n-1p + 1s = 1r - 1s\n2s = p + r\nThen setting these equal:\n\\mathbb{E}(\\text{R}) = \\mathbb{E}(\\text{S})\n-1p + 1s = -1r + 1p\ns + r = 2p\nAnd finally setting these equal:\n\\mathbb{E}(\\text{P}) = \\mathbb{E}(\\text{S})\n1r - 1s = -1r + 1p\n2r = s + p\nNow we have these equations:\n\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\nWe can rewrite the 1st:\nr = 2s - p\nAnd combine with the 2nd:\ns + (2s - p) = 2p\n3s = 3p\nResulting in:\ns = p\nNow we can go back to the 2nd equation:\ns + r = 2p\nAnd insert s = p:\ns + r = 2s\nAnd arrive at:\nr = s\nWe now see that all are equal:\ns = p = r\nWe also know that they must all sum to 1:\nr + p + s = 1\nSince they’re all equal and sum to 1, we can substitute p and s with r:\n3r = 1\nr = 1/3\nSo all actions are taken with probability 1/3:\nr = p = s = 1/3 \\quad \\blacksquare\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= r*0 + p*1 + s*-1 \\\\\n&= 1/3*0 + 1/3*1 + 1/3*-1 \\\\\n&= 0\n\\end{split}\n\\end{equation}",
    "crumbs": [
      "About",
      "RPS Theory"
    ]
  },
  {
    "objectID": "rpstheory.html#game-theory-equilibrium",
    "href": "rpstheory.html#game-theory-equilibrium",
    "title": "RPS Hackathon @ Recurse: Guide to Building a Bot",
    "section": "",
    "text": "start with GTO then exploit\nRPS is a zero-sum game and the payouts are symmetrical as follows:\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe Nash Equilibrium strategy is to play each action r = p = s = 1/3 of the time.\n\n\n\n\n\n\nNash Equilibrium Strategy for RPS\n\n\n\n\n\nIf Player 1 plays Rock with probability r, Paper with probability p, and Scissors with probability s, we have the following expected value equations for Player 2:\n\\mathbb{E}(\\text{R}) = -1p + 1s\n\\mathbb{E}(\\text{P}) = 1r - 1s\n\\mathbb{E}(\\text{S}) = -1r + 1p\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for r, p, and s, we can start by setting these EVs equal:\n\\mathbb{E}(\\text{R}) = \\mathbb{E}(\\text{P})\n-1p + 1s = 1r - 1s\n2s = p + r\nThen setting these equal:\n\\mathbb{E}(\\text{R}) = \\mathbb{E}(\\text{S})\n-1p + 1s = -1r + 1p\ns + r = 2p\nAnd finally setting these equal:\n\\mathbb{E}(\\text{P}) = \\mathbb{E}(\\text{S})\n1r - 1s = -1r + 1p\n2r = s + p\nNow we have these equations:\n\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\nWe can rewrite the 1st:\nr = 2s - p\nAnd combine with the 2nd:\ns + (2s - p) = 2p\n3s = 3p\nResulting in:\ns = p\nNow we can go back to the 2nd equation:\ns + r = 2p\nAnd insert s = p:\ns + r = 2s\nAnd arrive at:\nr = s\nWe now see that all are equal:\ns = p = r\nWe also know that they must all sum to 1:\nr + p + s = 1\nSince they’re all equal and sum to 1, we can substitute p and s with r:\n3r = 1\nr = 1/3\nSo all actions are taken with probability 1/3:\nr = p = s = 1/3 \\quad \\blacksquare\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= r*0 + p*1 + s*-1 \\\\\n&= 1/3*0 + 1/3*1 + 1/3*-1 \\\\\n&= 0\n\\end{split}\n\\end{equation}",
    "crumbs": [
      "About",
      "RPS Theory"
    ]
  },
  {
    "objectID": "rpstheory.html#submitting-bots",
    "href": "rpstheory.html#submitting-bots",
    "title": "RPS Hackathon @ Recurse: Guide to Building a Bot",
    "section": "Submitting Bots",
    "text": "Submitting Bots\nTo get started, click the Login button to the left and then go to Submit Your Bot. On the bot submission page, it’s possible to submit with:\n\nCode text boxes directly on the webpage\nFile upload a single player.py Python file",
    "crumbs": [
      "About",
      "RPS Theory"
    ]
  },
  {
    "objectID": "rpstheory.html#game-engine",
    "href": "rpstheory.html#game-engine",
    "title": "RPS Hackathon @ Recurse: Guide to Building a Bot",
    "section": "Game Engine",
    "text": "Game Engine\nIf you would like to clone the repository with the game engine, you can find it at https://github.com/pokercamp/rps-engine.\nIn players/default, the player.py file is where you write your bot. We don’t recommend changing any other files.\nThe engine is in engine.py. You can use engine.py to run two bots against each other. You can use this to test your bot against itself or other bots that you create.\nThe following code will run n_games between p1_name and p2_name and output the result to the specified output_dir.\nThe generic usage is:\npython3 engine.py -p1 {p1_name} {p1_file_path} -p2 {p2_name} {p2_file_path} -o {output_dir} -n {n_games}\"\nFor example, to run a 200 game match with two bots that are named p1 and p2 with files in the players/default/ folder and outputted to the p1p2test folder, do this:\npython3 engine.py -p1 'p1' players/default/ -p2 'p2' players/default/ -o p1p2test -n 200\nThe output files are in the folder p1p2test:\n\nscores.p1.p2.txt contains the raw scores of each player\nThe p1.p2 folder contains:\n\n\ngamelog.txt: A log of all hands played\nOther log files for each player",
    "crumbs": [
      "About",
      "RPS Theory"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html",
    "href": "rpsbotchallenge.html",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "",
    "text": "Separate weekly competition for non-1st time people (all non-1st time people + assortment of house bots)\nDaily RPS v1 is X number of games vs. one of the weekly competition bot entrants from the previous week (or from whenever, or also including occasional LLM or house bots). You can elect to play vs. the bot manually, with some-code, or full-code.\nDaily RPS v2 is “Medium level” from below, but can we sustain this? But also if we do #3 above is that going to get",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#strategy",
    "href": "rpsbotchallenge.html#strategy",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Strategy",
    "text": "Strategy\nYour mission is to play well against the environment bots without getting counterplayed by other participant bots.\n\nStrategy vs. House Bots\nThe histories that are shown against the bots will generally be very useful in discovering patterns. Combine this with the bot names, and you’ll be able to develop a strong strategy against these bots for future rounds.\n\n\nStrategy vs. Participant Bots\nLook for patterns. If participant bots are only trying to exploit environment bots, then they might be exploitable themselves.",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#writing-bots",
    "href": "rpsbotchallenge.html#writing-bots",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Writing Bots",
    "text": "Writing Bots\nClick here for our guide on getting started writing RPS bots.",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#schedule",
    "href": "rpsbotchallenge.html#schedule",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Schedule",
    "text": "Schedule\n2:00pm: Get set up\n2:15pm: Begin Round 1\n2:45pm: Round 1 results, begin Round 2\n3:15pm: Round 2 results, begin Round 3\n3:45pm: Round 3 results, begin Round 4\n4:15pm: Round 4 results, begin Round 5\n4:45pm: Round 5 results, finish up/hang out\n5:00pm: End",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "leaderboard.html",
    "href": "leaderboard.html",
    "title": "RPS Hackathon @ Recurse: Leaderboard",
    "section": "",
    "text": "Latest freeplay round\n\n\nLoading…\n\n\n\n\n\nLatest scoring round\n\n\nLoading…\n\n\n\n\n\nNext round\n\n\n\nAll scoring rounds\n\n\nLoading…"
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "RPS Hackathon @ Recurse: Tests and Logs",
    "section": "",
    "text": "Latest submission versus last scoring round\n\n\nLoading…\n\n\n\n\n\nNext round\n\nLoading…\n\nMatch logs\n\n\nLoading…"
  },
  {
    "objectID": "rpsom.html",
    "href": "rpsom.html",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "",
    "text": "In poker, many players start with GTO then exploit\nmaximizing reward while not being exploitable\nRPS opponent modeling going through stages of simple opponent given to complex mix of opponents prediction stuff\nFrom simple to Iocaine How to parametrize opponent Multiple hypotheses, run counterfactually, pick best one\nPattern hypotheses, compare to general pattern about something else Play against levels Take from rps bot spreadsheet LLM Add rps.bot as tutorial on opponent modeling focused on RPS that leads to our competitions\nIntro opponent modeling progression course (see high level/weekly challenge below) 1a) By end, you enter your bot into weekly competition for 1st time people (all 1st time people + assortment of house bots)\nBuild a single program that levels up as we give you progressively more difficult training games → enter into the weekly challenge by the end (or skip directly to that)\nRange of opponents: Static bad Static better Dynamic with no relation to game (e.g. RPSRPSRPS) RR: I think “dynamic with no relation to game” can be generalized to “static Markov model” where you have a hidden state, your hidden state completely defines your probabilities, and the round moves + outcome + your state makes you transition to a specific new state Dynamic only related to own or opponent actions Dynamic only related to recent actions Dynamic related to entire history of actions Adversarial actions, i.e. intelligently exploiting opponents\nRange of games: We tell you single opponent We tell you multiple opponents You detect single opponent You detect multiple opponents Enter weekly challenge of unknown multiple opponents + other human created bots from that week",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#how-it-works",
    "href": "rpsom.html#how-it-works",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "How it Works",
    "text": "How it Works\n\nRock Paper Scissors\n\n\n\nImage by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties\n\n\n\nCompetition Rounds\nThere will be 5 rounds of RPS competitions. Each will have:\n\n1 bot entered by each participant\n4 new house bots\n\nYou’ll play 200 games against each other bot twice.\nBefore each round, you’ll be given the next round’s name, which will be a hint about the strategies for all of those bots.\n\n\nHouse Bots\nEach house bot that enters during a round will stay for all future rounds.\nThe bot names reveal the bot strategies. The names will be anonymized during each round and then revealed afterwards (except for the final round where they will be revealed, but won’t be that useful!).\n\n\nSubmissions\n\nFor each round, you can submit as many times as you want up to the end of that round\nStarting in the 2nd round, each submission during the round will receive a test result based on how that bot would have done in the previous round\n\nDoing better in the previous round is a sign of progress, but may not actually result in a better score in the current round, which will have updated participant bots and new environment bots\n\nThe final submission before the end of the round will be entered into that round’s competition (and will stay active for future rounds as well until you submit a new one)\nAt the end of each round, we’ll run the tournament and show the results\n\n\n\nScoring\nAt the end of each round, you’ll get the following information:\n\nMatrix score report with results against every other bot\n\nYour score is reported as profit per 100 games\n\nGame histories for each match\n\nYour cumulative score is the sum of all of the round scores, where the first round is worth 1x and each future round increases by 1.3x (i.e., round 2 is 1.3x, round 3 is 1.69x, etc.).\n\n\nPrizes\nThere will be a small on-theme prize for the overall winner.",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#strategy",
    "href": "rpsom.html#strategy",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Strategy",
    "text": "Strategy\nYour mission is to play well against the environment bots without getting counterplayed by other participant bots.\n\nStrategy vs. House Bots\nThe histories that are shown against the bots will generally be very useful in discovering patterns. Combine this with the bot names, and you’ll be able to develop a strong strategy against these bots for future rounds.\n\n\nStrategy vs. Participant Bots\nLook for patterns. If participant bots are only trying to exploit environment bots, then they might be exploitable themselves.",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#writing-bots",
    "href": "rpsom.html#writing-bots",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Writing Bots",
    "text": "Writing Bots\nClick here for our guide on getting started writing RPS bots.",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#schedule",
    "href": "rpsom.html#schedule",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Schedule",
    "text": "Schedule\n2:00pm: Get set up\n2:15pm: Begin Round 1\n2:45pm: Round 1 results, begin Round 2\n3:15pm: Round 2 results, begin Round 3\n3:45pm: Round 3 results, begin Round 4\n4:15pm: Round 4 results, begin Round 5\n4:45pm: Round 5 results, finish up/hang out\n5:00pm: End",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#why",
    "href": "rpsom.html#why",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Why?",
    "text": "Why?\nFun! And thinking about strategy in a repeated game against a variety of opponents.\nFrom DeepMind in 2023:\n\nIn sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#starter-code",
    "href": "rpsom.html#starter-code",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Starter Code",
    "text": "Starter Code\nWe provide Python starter code that you can submit directly on the Submission page or that you can download and submit through file upload.\nWe provide code that by default plays randomly. The random strategy guarantees a breakeven result, which won’t be good enough to win because it won’t take advantage of the suboptimal house bots!\n\nVariables\nThe profit so far in the match is stored in:\n\nself.my_profit\n\nAnd the history of your actions and your opponent actions are stored in the following array, where each history is a tuple of (my_action, their_action).\n\nself.history[]\n\n\n\nFunctions\n\ndef __init__(self):\n\nInitializes profit self.my_profit\nInitializes the history array self.history[]\n\nhandle_results(self, *, my_action, their_action, my_payoff, match_clock):\n\nThis is run at the end of every game\nIt updates the history with my_action and their_action\nIt updates the payoff with my_payoff\nmatch_clock can be used to see how much of your 30 second total time is remaining (this can generally be ignored)\n\ndef get_action(self, *, match_clock):\n\nThis is the main function where you run your strategy and return an action\nYou can return RockAction(), PaperAction(), or ScissorsAction()",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#problem-solving-and-strategy",
    "href": "rpsom.html#problem-solving-and-strategy",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Problem Solving and Strategy",
    "text": "Problem Solving and Strategy\nThe goal of the hackathon is to figure out how to adapt to your opponents (other participants and house bots)!\nA few possible strategies are:\n\nSingle out specific bots that are exploitable and write code to identify and exploit them\n\nBuild an ensemble of exploits targeting many opponents\n\nAttempt to predict the next move of each bot using a general algorithm\nUse random play as a backup plan (see below for a full explanation of why this Nash equilibrium strategy always breaks even)",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#llms",
    "href": "rpsom.html#llms",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "LLMs",
    "text": "LLMs\nWe welcome you to use LLMs like Claude or ChatGPT. You can provide the starter code/code samples and get their help with implementing better strategies.",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#writing-your-own-bot",
    "href": "rpsom.html#writing-your-own-bot",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Writing Your Own Bot",
    "text": "Writing Your Own Bot\nYour bot will be facing off against a variety of participant bots and house bots. Here we illustrate some examples of how to write more complex bots. The main function to modify is get_action, which is the one that returns the move for each game.\n\nWin vs. Last Action\nThis bot plays to beat the opponent’s last action.\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      last_opponent_move = self.history[-1][1] # Find last opponent move\n      if isinstance(last_opponent_move, RockAction): # If Rock\n          return PaperAction() # Play Paper\n      elif isinstance(last_opponent_move, PaperAction): # If Paper\n          return ScissorsAction() # Play Scissors\n      else: # If Scissors\n          return RockAction() # Play Rock\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nWin vs. Opponent Playing “Win vs. Last Action”\nThis bot plays to beat the opponent playing the above “win vs. last action” strategy.\nIf our last action is Rock, they would play Paper, so we should play Scissors.\nIf our last action is Paper, they would play Scissors, so we should play Rock.\nIf our last action is Scissors, they would play Rock, so we should play Paper.\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      our_last_move = self.history[-1][0] # Find our last move\n      if isinstance(our_last_move, RockAction): # If Rock\n          return ScissorsAction() # Play Scissors\n      elif isinstance(our_last_move, PaperAction): # If Paper\n          return RockAction() # Play Rock\n      else: # If Scissors\n          return PaperAction() # Play Paper\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nWin vs. Full Opponent History Most Frequent\nInstead of just looking at the last action, this bot plays to beat the most frequent opponent action over the entire history.\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      opponent_moves = [move[1] for move in self.history]\n      \n      # Count the occurrences of each move\n      move_counts = {} # Count each R/P/S\n      for move in opponent_moves:\n          if move in move_counts:\n              move_counts[move] += 1 # Add to counter\n          else:\n              move_counts[move] = 1 # Start counter\n      \n      # Find the move with the highest count\n      most_common_move = None\n      highest_count = 0\n      for move, count in move_counts.items():\n          if count &gt; highest_count:\n              most_common_move = move\n              highest_count = count\n      \n      if isinstance(most_common_move, RockAction):\n          return PaperAction()\n      elif isinstance(most_common_move, PaperAction):\n          return ScissorsAction()\n      else: # most_common_move is ScissorsAction\n          return RockAction()\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nSometimes Playing Randomly\nSince there are many suboptimal environment bots, breaking even by playing randomly won’t be a good enough overall strategy, but could be valuable to consider in some situations. Here is an example of using random play in combination with another strategy.\nOne of the bots above is Win vs. Last Action. Suppose that you have a theory that you should actually play to beat the action that they played two games ago.\nYou could try something like this:\n\nIf you’re winning or tying, then play to beat the action from two games ago 80% of the time and play randomly 20% of the time so that you aren’t too predictable\nIf you’re losing (i.e. maybe this strategy is not working well), then always play randomly\n\ndef get_action(self, *, match_clock):\n\n  # If losing, play randomly\n  if self.my_profit &lt; 0:\n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n  # If winning or tying, and there are at least 2 rounds of history\n  if len(self.history) &gt;= 2:\n      # 80% chance to play based on opponent's action from 2 rounds ago\n      if random.random() &lt; 0.8:\n          two_rounds_ago = self.history[-2][1]  # Opponent's action from 2 rounds ago\n          if isinstance(two_rounds_ago, RockAction):\n              return PaperAction()\n          elif isinstance(two_rounds_ago, PaperAction):\n              return ScissorsAction()\n          else:  # ScissorsAction\n              return RockAction()\n\n  # In all other cases (including 1st 2 rounds), play randomly\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nExploiting a Weak Bot\nIn practice in this hackathon, a lot of value will come from exploiting weak bots or other opponents.\nFor example, you could check to see if any bot is always playing Rock. If so, you always play Paper. If not, you play randomly. Then you will breakeven against everyone, but crush the Rock-only bot.\nHere’s how to execute this strategy:\ndef get_action(self, *, match_clock):\n  if self.history: # Check if the opponent has played at least once\n      # Get all of the opponent's moves\n      opponent_moves = [move[1] for move in self.history]\n      \n      # Count how many times the opponent played Rock\n      rock_count = sum(1 for move in opponent_moves if isinstance(move, RockAction))\n      \n      # If opponent has only played Rock, we play Paper\n      if rock_count == len(opponent_moves):\n          return PaperAction()\n\n  # If it's the first move or opponent hasn't only played Rock, play randomly\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])",
    "crumbs": [
      "About",
      "RPS Opponent Modeling"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "",
    "text": "Image by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#how-it-works",
    "href": "intro.html#how-it-works",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "",
    "text": "Image by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#why",
    "href": "intro.html#why",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Why?",
    "text": "Why?\nWe think RPS is fun and while simple enough that everyone knows it and understands the rules, it’s still a great domain for thinking about strategy in a repeated game against a variety of opponents.",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#game-theory-equilibrium",
    "href": "intro.html#game-theory-equilibrium",
    "title": "RPS Hackathon @ Recurse: Hackathon Instructions",
    "section": "Game Theory Equilibrium",
    "text": "Game Theory Equilibrium\n\nPayoff matrix\nThe core features of a game are its players, the actions of each player, and the payoffs. We can show this for RPS in the below payoff matrix, also known as normal-form.\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe payoffs for Player 1 are on the left and for Player 2 on the right in each payoff outcome of the game. For example, the bottom left payoff is when Player 1 plays Scissors and Player 2 plays Rock, resulting in -1 for P1 and +1 for P2.\nA strategy says which actions you would take for every state of the game.\n\n\nExpected value\nExpected value in a game represents the average outcome of a strategy if it were repeated many times. It’s calculated by multiplying each possible outcome by its probability of occurrence and then summing these products.\nSuppose that Player 1 plays the strategy:\n\n\\begin{cases}\nr_1 = 0.5 \\\\\np_1 = 0.25 \\\\\ns_1 = 0.25\n\\end{cases}\n\nand Player 2 plays the strategy:\n\n\\begin{cases}\nr_2 = 0.1 \\\\\np_2 = 0.3 \\\\\ns_2 = 0.6\n\\end{cases}\n\nLet’s add these to the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p_1=0.25)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s_1=0.25)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nTo simplify, let’s just write the payoffs for Player 1 since payoffs for Player 2 will simply be the opposite:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n0\n-1\n1\n\n\nPaper (p_1=0.25)\n1\n0\n-1\n\n\nScissors (s_1=0.25)\n-1\n1\n0\n\n\n\nNow we can multiply the player action strategies together to get a percentage occurrence for each payoff in the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\nVal: 0 Pr: 0.5(0.1) = .05\nVal: -1 Pr: 0.5(0.3) = .15\nVal: 1 Pr: 0.5(0.6) = .3\n\n\nPaper (p_1=0.25)\nVal: 1 Pr: 0.25(0.1) = .025\nVal: 0 Pr: 0.25(0.3) = .075\nVal: -1 Pr: 0.25(0.6) = .15\n\n\nScissors (s_1=0.25)\nVal: -1 Pr: 0.25(0.1) = .025\nVal: 1 Pr: 0.25(0.3) = .075\nVal: 0 Pr: 0.25(0.6) = .15\n\n\n\nNote that the total probabilities sum to 1 and each row and column sums to the probability of playing that row or column.\nWe can work out the expected value of the game to Player 1 (summing all payoffs multiplied by probabilities from top left to bottom right):\n\\mathbb{E}[P_1] = 0(0.05) + -1(0.15) + 1(0.3) + 1(0.025) + 0(0.075) + -1(0.15) + -1(0.025) + 1(0.075) + 0(0.15) = 0.075\nTherefore P1 is expected to gain 0.075 per game given these strategies. Since payoffs are reversed for P2, P2’s expectation is -0.075 per game.\n\n\nZero-sum\nWe see in the matrix that every payoff is zero-sum, i.e. the sum of the payoffs to both players is 0. This means the game is one of pure competition. Any amount P1 wins is from P2 and vice versa.\n\n\nNash equilibrium\nA Nash equilibrium means that no player can improve their expected payoff by unilaterally changing their strategy. That is, changing one’s strategy can only result in the same or worse payoff (assuming the other player does not change).\nIn RPS, the Nash equilibrium strategy is to play each action r = p = s = 1/3 of the time. I.e., to play totally randomly.\nPlaying a combination of strategies is called a mixed strategy, as opposed to a pure strategy, which would select only one action. Mixed strategies are useful in games of imperfect information because it’s valuable to not be predictable and to conceal your private information. In perfect information games, the theoretically optimal play would not contain any mixing (i.e., if you could calculate all possible moves to the end of the game).\nThe equilibrium RPS strategy is worked out below:\n\n\n\n\n\n\nNash equilibrium strategy for RPS\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock (r)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nIf Player 1 plays Rock with probability r, Paper with probability p, and Scissors with probability s, we have the following expected value equations for Player 2:\n\\mathbb{E}[P_2(R)] = 0r -1p + 1s\n\\mathbb{E}[P_2(P)] = 1r + 0p - 1s\n\\mathbb{E}[P_2(S)] = -1r + 1p + 0s\n(To compute each of these, sum the payoffs for each column with P2 payoffs and P1 probabilities. P2 payoffs because these are the expected values for P2 and P1 probabiltiies because the payoffs depend on the strategy probabilties of P1 against each of P2’s actions.)\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for r, p, and s, we can start by setting these EVs equal:\n\\mathbb{E}[P_2(R)] = \\mathbb[P_2(P)]\n-1p + 1s = 1r - 1s\n2s = p + r\nThen setting these equal:\n\\mathbb{E}[P_2(R)] = \\mathbb{E}[P_2(S)]\n-1p + 1s = -1r + 1p\ns + r = 2p\nAnd finally setting these equal:\n\\mathbb{E}[P_2(P)] = \\mathbb{E}[P_2(S)]\n1r - 1s = -1r + 1p\n2r = s + p\nNow we have these equations:\n\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\nWe can rewrite the 1st:\nr = 2s - p\nAnd combine with the 2nd:\ns + (2s - p) = 2p\n3s = 3p\nResulting in:\ns = p\nNow we can go back to the 2nd equation:\ns + r = 2p\nAnd insert s = p:\ns + r = 2s\nAnd arrive at:\nr = s\nWe now see that all are equal:\ns = p = r\nWe also know that they must all sum to 1:\nr + p + s = 1\nSince they’re all equal and sum to 1, we can substitute p and s with r:\n3r = 1\nr = 1/3\nSo all actions are taken with probability 1/3:\nr = p = s = 1/3 \\quad \\blacksquare\nBy symmetry, the same equilibrium strategy is true for Player 2.\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= 0(r) + 1(p) + -1(s) \\\\\n&= 0(1/3) + 1(1/3) + -1(1/3) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nHow about the case of the opponent playing 60% Rock, 20% Paper, 20% Scissors?\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. 622}) &= 0.6(\\text{Equilibrium vs. Rock}) \\\\\n&\\quad{}+ 0.2(\\text{Equilibrium vs. Paper}) \\\\  \n&\\quad{}+ 0.2(\\text{Equilibrium vs. Scissors}) \\\\\n&= 0.6(0) + 0.2(0) + 0.2(0) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nThe random equilibrium strategy will result in 0 against any pure strategy and any combination of strategies including 622 and the opponent playing the random strategy.\n\n\nExploiting vs. Nash\nThe equilibrium strategy vs. a pure Rock opponent is a useful illustration of the limitations of playing at equilibrium. The Rock opponent is playing the worst possible strategy, yet equilibrium is still breaking even!\nWhat’s the best that we could do against Rock only? We could play purely paper. This is called a best response strategy. The payoffs are written for playing Paper and the probabilities indicate the opponent playing only Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. Rock}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(1) + 0(0) + -1(0) \\\\\n&= 1\n\\end{split}\n\\end{equation}\n\nWe’d win 1 each game playing Paper vs. Rock.\nHow about against the opponent playing 60% Rock, 20% Paper, 20% Scissors? Here we can see that because they are overplaying Rock, our best strategy is again to always play Paper. We write the payoffs for playing Paper and the probabilities according to the 622 strategy.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. 622}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(0.6) + 0(0.2) + -1(0.2) \\\\\n&= 0.6 + 0 - 0.2 \\\\\n&= 0.4\n\\end{split}\n\\end{equation}\n\nPlaying Paper vs. 622 results in an expected win of 0.4 per game.",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RPS.bot",
    "section": "",
    "text": "RPS.bot is an intro to opponent modeling through the game Rock Paper Scissors.\nGet started now! Start with Intro to RPS.\nAlthough seemingly very simple, RPS is valuable for thinking about strategy in a repeated game against a variety of opponents.\nFrom DeepMind in 2023:\n\nIn sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.\n\n\n\ngonna 3d print the exploitability of a player's rock-paper-scissors strategy over their strategy simplex pic.twitter.com/cSXFKyI5E8\n\n— Kevin a. Wang (@often_wang) September 24, 2024",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "RPS.bot: About",
    "section": "",
    "text": "RPS.bot by Max Chiswick and Ross Rheingans-Yoo.\nWe’re excited about learning through games. Check out Overbet.ai (under development) and Poker Camp for more."
  },
  {
    "objectID": "about.html#about-us",
    "href": "about.html#about-us",
    "title": "RPS.bot: About",
    "section": "",
    "text": "RPS.bot by Max Chiswick and Ross Rheingans-Yoo.\nWe’re excited about learning through games. Check out Overbet.ai (under development) and Poker Camp for more."
  },
  {
    "objectID": "botsubmit.html",
    "href": "botsubmit.html",
    "title": "RPS Hackathon @ Recurse: Submit",
    "section": "",
    "text": "Edit Code\n\n\nUpload File\n\n\nLink Repo\n\n\n\n\nSubmit\n\n\n\n\n\n\n\n\n\n\n\nUpload File\n\n\n\n\nThe file you upload should be a Python file that looks something like this. You can download this file in the rps-engine repo at players/default/player.py and for more info on testing see: Game Engine.\n# Simple example bot, written in Python.\n\nfrom skeleton.actions import RockAction, PaperAction, ScissorsAction\nfrom skeleton.bot import Bot\nfrom skeleton.runner import parse_args, run_bot\n\nimport random\n\nclass Player(Bot):\n    # A bot for playing Rock-Paper-Scissors.\n\n    def __init__(self):\n        # Called when a new matchup starts. Called exactly once.\n        \n        self.my_profit = 0\n        self.history = []\n\n    def handle_results(self, *, my_action, their_action, my_payoff, match_clock):\n        # Called after a round. Called NUM_ROUNDS times.\n        \n        self.history.append((my_action, their_action))\n        self.my_profit += my_payoff\n\n    def get_action(self, *, match_clock):\n        # Where the magic happens. Called when the engine needs an action from\n        # your bot. Called NUM_ROUNDS times.\n        #\n        # Returns a RockAction(), PaperAction(), or ScissorsAction().\n        \n        return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\nif __name__ == '__main__':\n    run_bot(Player(), parse_args())\n\n\n\nComing soon! Ask if you’d like to use this to upload multiple files or something else that doesn’t fit into a player.py file."
  }
]