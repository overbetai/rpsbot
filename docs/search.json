[
  {
    "objectID": "rpsomtutorial.html",
    "href": "rpsomtutorial.html",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "",
    "text": "For the tutorial, suppose that you are entering 200-game matches in various scenarios. The scenarios lead to building a progressively more complex bot as you encounter progressively more difficult challenges.",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "rpsomtutorial.html#beat-a-fixed-simple-unknown-opponent",
    "href": "rpsomtutorial.html#beat-a-fixed-simple-unknown-opponent",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "Beat a fixed simple unknown opponent",
    "text": "Beat a fixed simple unknown opponent\nIn practice, a lot of value will come from exploiting weak bots or other opponents.\nIf you know that your opponent is fixed and has a simple strategy, then you can:\n\nStart by playing random\nSee what happens\nBeat simple opponent after you see what they’re doing in the first few games\n\nAutomate this",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "rpsomtutorial.html#beat-multiple-opponents-with-one-bot",
    "href": "rpsomtutorial.html#beat-multiple-opponents-with-one-bot",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "Beat multiple opponents with one bot",
    "text": "Beat multiple opponents with one bot\nSuppose that you have three sequential opponents: 1. Rock only 2. Beat last move 3. Beat last move\nWhat happens if we run our last bot in this scenario? We’d crush the Rock only bot, but then get crushed ourselves by the Beat last move bots. Our EV would be 100 - 99 - 99 = -98!\nWell then we could do much better if we decided to beat the Beat last move bots instead of the Rock only bot. To beat them, we’d play:\n\nIf last move Paper, opponent plays Scissors, we play Rock\nIf last move Rock, opponent plays Paper, we play Scissors\nIf last move Scissors, opponent plays Rock, we play Paper\n\nThis means that if we play Rock on the first move, we’d forever play RSPRSPRSP… (similarly if we played Paper on the first move, we’d forever play PRSPRSPRS…)\nIf we cycle, that means that we breakeven vs. Rock only. Exploiting and not getting exploited, but not fully exploiting.",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "rpsomtutorial.html#beat-multiple-opponents-with-one-bot-1",
    "href": "rpsomtutorial.html#beat-multiple-opponents-with-one-bot-1",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "Beat multiple opponents with one bot",
    "text": "Beat multiple opponents with one bot\n\nRock only\nBeat last move\nFinal bossbot\n\nAssume that the final bossbot keeps a dictionary of your actions after each of the following action pairs, and then plays the move to beat those.\nRR PP SS RP RS PR PS SR SP\nThat seems difficult to beat! Although worth thinking about, let’s prioritize beating the simpler bots that we already know how to beat.",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "rpsomtutorial.html#read-which-opponent-youre-playing-and-switch-to-ensemble-selection",
    "href": "rpsomtutorial.html#read-which-opponent-youre-playing-and-switch-to-ensemble-selection",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "Read which opponent you’re playing and switch to ensemble selection",
    "text": "Read which opponent you’re playing and switch to ensemble selection\nRead history and give last 10 Determine if playing vs. A, B, or C",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "rpsomtutorial.html#gto-deviation",
    "href": "rpsomtutorial.html#gto-deviation",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "GTO deviation",
    "text": "GTO deviation\nAssume by default that the opponent is playing approximately GTO and then detect deviations and play accordingly dirichlet What’s the weakness here? This is only detecting probabilities and not patterns.",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "rpsomtutorial.html#n-gram",
    "href": "rpsomtutorial.html#n-gram",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "N-gram",
    "text": "N-gram\nN-gram models of opponents most recent same history Always asking does this model work? Playing to beat them if it does Look at a bunch of models, which has best predictive power against opponent, play that one How to parametrize opponent Multiple hypotheses, run counterfactually, pick best one\nPattern hypotheses, compare to general pattern about something else Iocaine Powder",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "rpsomtutorial.html#challenges",
    "href": "rpsomtutorial.html#challenges",
    "title": "RPS.bot: Opponent Modeling Tutorial",
    "section": "Challenges",
    "text": "Challenges\nSee our Challenge page to get your bots into action in our:\n\nRock Practice Gym\nDaily Challenge\nWeekly Challenge\nOne-off Hackathons\n\nThere’s a special Weekly Challenge for first-timers.",
    "crumbs": [
      "About",
      "Opponent Modeling Tutorial"
    ]
  },
  {
    "objectID": "botsubmit.html",
    "href": "botsubmit.html",
    "title": "RPS Hackathon @ Recurse: Submit",
    "section": "",
    "text": "Edit Code\n\n\nUpload File\n\n\nLink Repo\n\n\n\n\nSubmit\n\n\n\n\n\n\n\n\n\n\n\nUpload File\n\n\n\n\nThe file you upload should be a Python file that looks something like this. You can download this file in the rps-engine repo at players/default/player.py and for more info on testing see: Game Engine.\n# Simple example bot, written in Python.\n\nfrom skeleton.actions import RockAction, PaperAction, ScissorsAction\nfrom skeleton.bot import Bot\nfrom skeleton.runner import parse_args, run_bot\n\nimport random\n\nclass Player(Bot):\n    # A bot for playing Rock-Paper-Scissors.\n\n    def __init__(self):\n        # Called when a new matchup starts. Called exactly once.\n        \n        self.my_profit = 0\n        self.history = []\n\n    def handle_results(self, *, my_action, their_action, my_payoff, match_clock):\n        # Called after a round. Called NUM_ROUNDS times.\n        \n        self.history.append((my_action, their_action))\n        self.my_profit += my_payoff\n\n    def get_action(self, *, match_clock):\n        # Where the magic happens. Called when the engine needs an action from\n        # your bot. Called NUM_ROUNDS times.\n        #\n        # Returns a RockAction(), PaperAction(), or ScissorsAction().\n        \n        return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\nif __name__ == '__main__':\n    run_bot(Player(), parse_args())\n\n\n\nComing soon! Ask if you’d like to use this to upload multiple files or something else that doesn’t fit into a player.py file."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "RPS.bot: About",
    "section": "",
    "text": "RPS.bot by Max Chiswick and Ross Rheingans-Yoo.\nWe’re excited about learning through games. Check out Overbet.ai (under development) and Poker Camp for more."
  },
  {
    "objectID": "about.html#about-us",
    "href": "about.html#about-us",
    "title": "RPS.bot: About",
    "section": "",
    "text": "RPS.bot by Max Chiswick and Ross Rheingans-Yoo.\nWe’re excited about learning through games. Check out Overbet.ai (under development) and Poker Camp for more."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RPS.bot",
    "section": "",
    "text": "RPS.bot is an intro to opponent modeling through the game Rock Paper Scissors.\nGet started now with Intro to RPS.",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "RPS.bot: Intro",
    "section": "",
    "text": "Image by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#rules",
    "href": "intro.html#rules",
    "title": "RPS.bot: Intro",
    "section": "",
    "text": "Image by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#why",
    "href": "intro.html#why",
    "title": "RPS.bot: Intro",
    "section": "Why?",
    "text": "Why?\nWe think RPS is fun and while it’s simple enough that everyone knows it and understands the rules, it’s still a great domain for thinking about strategy in a repeated game against a variety of opponents.\nFrom DeepMind in 2023:\n\nIn sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.\n\n\n\ngonna 3d print the exploitability of a player’s rock-paper-scissors strategy over their strategy simplex pic.twitter.com/cSXFKyI5E8\n\n— Kevin a. Wang (@often_wang) September 24, 2024",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "intro.html#rps-game-theory",
    "href": "intro.html#rps-game-theory",
    "title": "RPS.bot: Intro",
    "section": "RPS game theory",
    "text": "RPS game theory\n\nPayoff matrix\nThe core features of a game are its players, the actions of each player, and the payoffs. We can show this for RPS in the below payoff matrix, also known as normal-form.\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe payoffs for Player 1 are on the left and for Player 2 on the right in each payoff outcome of the game. For example, the bottom left payoff is when Player 1 plays Scissors and Player 2 plays Rock, resulting in -1 for P1 and +1 for P2.\nA strategy says which actions you would take for every state of the game.\n\n\nExpected value\nExpected value in a game represents the average outcome of a strategy if it were repeated many times. It’s calculated by multiplying each possible outcome by its probability of occurrence and then summing these products.\nSuppose that Player 1 plays the strategy:\n\n\\begin{cases}\nr_1 = 0.5 \\\\\np_1 = 0.25 \\\\\ns_1 = 0.25\n\\end{cases}\n\nand Player 2 plays the strategy:\n\n\\begin{cases}\nr_2 = 0.1 \\\\\np_2 = 0.3 \\\\\ns_2 = 0.6\n\\end{cases}\n\nLet’s add these to the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p_1=0.25)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s_1=0.25)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nTo simplify, let’s just write the payoffs for Player 1 since payoffs for Player 2 will simply be the opposite:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n0\n-1\n1\n\n\nPaper (p_1=0.25)\n1\n0\n-1\n\n\nScissors (s_1=0.25)\n-1\n1\n0\n\n\n\nNow we can multiply the player action strategies together to get a percentage occurrence for each payoff in the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\nVal: 0 Pr: 0.5(0.1) = .05\nVal: -1 Pr: 0.5(0.3) = .15\nVal: 1 Pr: 0.5(0.6) = .3\n\n\nPaper (p_1=0.25)\nVal: 1 Pr: 0.25(0.1) = .025\nVal: 0 Pr: 0.25(0.3) = .075\nVal: -1 Pr: 0.25(0.6) = .15\n\n\nScissors (s_1=0.25)\nVal: -1 Pr: 0.25(0.1) = .025\nVal: 1 Pr: 0.25(0.3) = .075\nVal: 0 Pr: 0.25(0.6) = .15\n\n\n\nNote that the total probabilities sum to 1 and each row and column sums to the probability of playing that row or column.\nWe can work out the expected value of the game to Player 1 (summing all payoffs multiplied by probabilities from top left to bottom right):\n\\mathbb{E}[P_1] = 0(0.05) + -1(0.15) + 1(0.3) + 1(0.025) + 0(0.075) + -1(0.15) + -1(0.025) + 1(0.075) + 0(0.15) = 0.075\nTherefore P1 is expected to gain 0.075 per game given these strategies. Since payoffs are reversed for P2, P2’s expectation is -0.075 per game.\n\n\nZero-sum\nWe see in the matrix that every payoff is zero-sum, i.e. the sum of the payoffs to both players is 0. This means the game is one of pure competition. Any amount P1 wins is from P2 and vice versa.\n\n\nNash equilibrium\nA Nash equilibrium means that no player can improve their expected payoff by unilaterally changing their strategy. That is, changing one’s strategy can only result in the same or worse payoff (assuming the other player does not change).\nIn RPS, the Nash equilibrium strategy is to play each action r = p = s = 1/3 of the time. I.e., to play totally randomly.\nPlaying a combination of strategies is called a mixed strategy, as opposed to a pure strategy, which would select only one action. Mixed strategies are useful in games of imperfect information because it’s valuable to not be predictable and to conceal your private information. In perfect information games, the theoretically optimal play would not contain any mixing (i.e., if you could calculate all possible moves to the end of the game).\nThe equilibrium RPS strategy is worked out below:\n\n\n\n\n\n\nNash equilibrium strategy for RPS\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock (r)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nIf Player 1 plays Rock with probability r, Paper with probability p, and Scissors with probability s, we have the following expected value equations for Player 2:\n\\mathbb{E}[P_2(R)] = 0r -1p + 1s\n\\mathbb{E}[P_2(P)] = 1r + 0p - 1s\n\\mathbb{E}[P_2(S)] = -1r + 1p + 0s\n(To compute each of these, sum the payoffs for each column with P2 payoffs and P1 probabilities. P2 payoffs because we are calculating the expected payoffs for P2 and P1 probabilities because the payoffs depend on the strategy probabilties of P1 against each of P2’s actions.)\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for r, p, and s, we can start by setting these EVs equal:\n\\mathbb{E}[P_2(R)] = \\mathbb{E}[P_2(P)]\n-1p + 1s = 1r - 1s\n2s = p + r\nThen setting these equal:\n\\mathbb{E}[P_2(R)] = \\mathbb{E}[P_2(S)]\n-1p + 1s = -1r + 1p\ns + r = 2p\nAnd finally setting these equal:\n\\mathbb{E}[P_2(P)] = \\mathbb{E}[P_2(S)]\n1r - 1s = -1r + 1p\n2r = s + p\nNow we have these equations:\n\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\nWe can rewrite the 1st:\nr = 2s - p\nAnd combine with the 2nd:\ns + (2s - p) = 2p\n3s = 3p\nResulting in:\ns = p\nNow we can go back to the 2nd equation:\ns + r = 2p\nAnd insert s = p:\ns + r = 2s\nAnd arrive at:\nr = s\nWe now see that all are equal:\ns = p = r\nWe also know that they must all sum to 1:\nr + p + s = 1\nSince they’re all equal and sum to 1, we can substitute p and s with r:\n3r = 1\nr = 1/3\nSo all actions are taken with probability 1/3:\nr = p = s = 1/3 \\quad \\blacksquare\nBy symmetry, the same equilibrium strategy is true for Player 2.\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= 0(r) + 1(p) + -1(s) \\\\\n&= 0(1/3) + 1(1/3) + -1(1/3) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nHow about the case of the opponent playing 60% Rock, 20% Paper, 20% Scissors?\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. 622}) &= 0.6(\\text{Equilibrium vs. Rock}) \\\\\n&\\quad{}+ 0.2(\\text{Equilibrium vs. Paper}) \\\\  \n&\\quad{}+ 0.2(\\text{Equilibrium vs. Scissors}) \\\\\n&= 0.6(0) + 0.2(0) + 0.2(0) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nThe random equilibrium strategy will result in 0 against any pure strategy and any combination of strategies including 622 and the opponent playing the random strategy.\n\n\nExploiting vs. Nash\nThe equilibrium strategy vs. a pure Rock opponent is a useful illustration of the limitations of playing at equilibrium. The Rock opponent is playing perhaps the worst possible strategy, yet equilibrium is still breaking even against it!\nWhat’s the best that we could do against Rock only? We could play purely paper. This is called a best response strategy. The payoffs are written for playing Paper and the probabilities indicate the opponent playing only Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. Rock}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(1) + 0(0) + -1(0) \\\\\n&= 1\n\\end{split}\n\\end{equation}\n\nWe’d win 1 each game playing Paper vs. Rock. Though now always playing Paper means that the opponent could always play Scissors and then we’d become the loser.\nHow about against the opponent playing 60% Rock, 20% Paper, 20% Scissors? Here we can see that because they are overplaying Rock, our best strategy is again to always play Paper. We write the payoffs for playing Paper and the probabilities according to the 622 strategy.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. 622}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(0.6) + 0(0.2) + -1(0.2) \\\\\n&= 0.6 + 0 - 0.2 \\\\\n&= 0.4\n\\end{split}\n\\end{equation}\n\nPlaying Paper vs. 622 results in an expected win of 0.4 per game.",
    "crumbs": [
      "About",
      "Intro to RPS"
    ]
  },
  {
    "objectID": "rpsom.html",
    "href": "rpsom.html",
    "title": "RPS.bot: Opponent Modeling Basics",
    "section": "",
    "text": "At the heart of opponent modeling is predicting your opponent’s next move. While  play is an interesting mathematical exercise, it’s often not actually the most rewarding strategy in practice.\nHumans tend to be predictably irrational. This can manifest itself as someone having favorite preferred moves, making “tilted” decisions, reacting to previous games in predictable ways, or having “leaks” like never throwing the same move three times in a row.\nThere can be levels to the game – maybe you think your opponent expects you to play Rock, so then you expect that they’ll play Paper, so you should play Scissors, but maybe they’ll realize this and play Rock to counter the Scissors, so you should actually play Paper, and so on.\nWe use suboptimal bots in our challenge games to reinforce thinking strategically about how to respond to opponents. The overall goal is to maximize payoffs, which is in practice taking advantage of opponent weaknesses while minimizing your own exploitability.",
    "crumbs": [
      "About",
      "Intro to Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#axes-of-opponent-modeling",
    "href": "rpsom.html#axes-of-opponent-modeling",
    "title": "RPS.bot: Opponent Modeling Basics",
    "section": "Axes of opponent modeling",
    "text": "Axes of opponent modeling\nThere are two main axes for opponent modeling: opponents and games.\n\nTypes of opponents\nHere are a few types of opponents and an example for each.\nBad static opponents: Playing Rock\nBetter static opponents: Playing 60% Rock, 20% Paper, 20% Scissors\nDynamic with no relation to game, i.e. a static Markov model: Playing RPSRPSRPS…\n\nDynamic only related to a limited subset of actions, like:\n\nOnly our own actions: Randomly play an action that we didn’t play in the last 2 games\nOnly opponent actions: Play the action that beats the most frequent of the opponent’s last 3 moves\nOnly most recent action: Copy the opponent’s last move\n\nSimple dynamic related to entire history of actions: Play the move that beats the most frequent opponent move so far\nAgent that switches its main strategy: An agent that plays all Rock for the first 20 games and then all Scissors for the next 20 games and then plays the move that beats the most frequent opponent move so far\nAdvanced dynamic related to entire history of actions: Sophisticated adversarial prediction algorithm or set of algorithms\n\n\nTypes of games/knowledge\nPlay a single known opponent (known meaning you know their entire strategy)\nPlay multiple known opponents\nPlay a single unknown opponent\nPlay multiple unknown opponents\nPlay multiple unknown opponents that include some suboptimal house bots and some human-created bots",
    "crumbs": [
      "About",
      "Intro to Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#starter-code",
    "href": "rpsom.html#starter-code",
    "title": "RPS.bot: Opponent Modeling Basics",
    "section": "Starter Code",
    "text": "Starter Code\nWe provide Python starter code that you can use to build RPS bots.\nThe code by default plays randomly, which will guarantee a breakeven result (and won’t be very fun).\nThe main actions are in the get_action function:\ndef get_action(self, *, match_clock):\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\nVariables\nThe profit so far in the match is stored in:\n\nself.my_profit\n\nAnd the history of your actions and your opponent actions are stored in the following array, where each history is a tuple of (my_action, their_action).\n\nself.history[]\n\n\n\nFunctions\n\ndef __init__(self):\n\nInitializes profit self.my_profit\nInitializes the history array self.history[]\n\nhandle_results(self, *, my_action, their_action, my_payoff, match_clock):\n\nThis is run at the end of every game\nIt updates the history with my_action and their_action\nIt updates the payoff with my_payoff\nmatch_clock is used to see how much of your 30 second total time is remaining (this can be ignored)\n\ndef get_action(self, *, match_clock):\n\nThis is the main function where you run your strategy and return an action\nYou can return RockAction(), PaperAction(), or ScissorsAction()",
    "crumbs": [
      "About",
      "Intro to Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#beating-simple-known-opponents",
    "href": "rpsom.html#beating-simple-known-opponents",
    "title": "RPS.bot: Opponent Modeling Basics",
    "section": "Beating simple known opponents",
    "text": "Beating simple known opponents\nIt’s a lot easier to beat your opponents when you know their strategy! Though unrealistic, this is still a worthwhile first exercise for thinking about which strategies beat which opponent strategies and how to code them.\n\nRockbot\nOpponent strategy: 100% Rock\nOur counter-strategy: 100% Paper\nOur best EV: +1 per game\nOur counter-strategy in code:\ndef get_action(self, *, match_clock):\n  return PaperAction()\n\n\nr532bot\nOpponent strategy: 50% Rock, 30% Paper, 20% Scissors\nOur counter-strategy: 100% Paper\nOur best EV: 0.5(1) + 0.3(0) + 0.2(-1) = 0.3 per game\nOur counter-strategy in code:\ndef get_action(self, *, match_clock):\n  return PaperAction()\n\n\nRPSbot\nOpponent strategy: RPSRPSRPS…\nOur counter-strategy: PSRPSRPSR…\nOur best EV: +1 per game\nOur ounter-strategy in code:\ndef get_action(self, *, match_clock):\n    moves = [PaperAction(), ScissorsAction(), RockAction()]\n    move_index = len(self.history) % 3  # Use history length to cycle\n    return moves[move_index]\n\n\nMimicbot\nOpponent strategy: Copy our last action\nOur counter-strategy:\n\nIf our last action was Rock, play Paper\nIf our last action was Paper, play Scissors\nIf our last action was Scissors, play Rock\n\nOur best EV: +1 per game (after 1st)\nOur counter-strategy code:\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      our_last_move = self.history[-1][0] # Find our last move\n      if isinstance(our_last_move, RockAction): # If Rock\n          return PaperAction() # Play Paper\n      elif isinstance(our_last_move, PaperAction): # If Paper\n          return ScissorsAction() # Play Scissors\n      else: # If Scissors\n          return RockAction() # Play Rock\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nBeatprevbot\nOpponent strategy: Beat our last action\nOpponent strategy code:\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      last_opponent_move = self.history[-1][1] # Find last opponent move\n      if isinstance(last_opponent_move, RockAction): # If Rock\n          return PaperAction() # Play Paper\n      elif isinstance(last_opponent_move, PaperAction): # If Paper\n          return ScissorsAction() # Play Scissors\n      else: # If Scissors\n          return RockAction() # Play Rock\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\nOur counter-strategy:\n\nIf our last action was Rock, play Scissors\nIf our last action was Paper, play Rock\nIf our last action was Scissors, play Paper\n\nOur best EV: +1 per game (after 1st)\nOur counter-strategy code:\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      our_last_move = self.history[-1][0] # Find our last move\n      if isinstance(our_last_move, RockAction): # If Rock\n          return ScissorsAction() # Play Scissors\n      elif isinstance(our_last_move, PaperAction): # If Paper\n          return RockAction() # Play Rock\n      else: # If Scissors\n          return PaperAction() # Play Paper\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\nThere could be more levels to this! If you assume that your opponent knows that you are going to do this, then they might play the next level up and instead of “beat previous”, go to “beat the move that beats the move that beats previous”.\nWe play: Paper Beat previous plays: Scissors Beat the move that beats the move that beats previous plays: Paper\nWe play: Rock Beat previous plays: Paper Beat the move that beats the move that beats previous plays: Rock\nWe play: Paper Beat previous plays: Scissors Beat the move that beats the move that beats previous plays: Paper\nThis turns out to be mimicbot, which we saw in the previous section! And more levels are possible…\n\n\nFreqbot\nOpponent strategy: Beat our most frequent overall action\nOpponent strategy code:\ndef get_action(self, *, match_clock):\n  if self.history: # After the 1st game\n      opponent_moves = [move[1] for move in self.history]\n      \n      # Count the occurrences of each move\n      move_counts = {} # Count each R/P/S\n      for move in opponent_moves:\n          if move in move_counts:\n              move_counts[move] += 1 # Add to counter\n          else:\n              move_counts[move] = 1 # Start counter\n      \n      # Find the move with the highest count\n      most_common_move = None\n      highest_count = 0\n      for move, count in move_counts.items():\n          if count &gt; highest_count:\n              most_common_move = move\n              highest_count = count\n      \n      if isinstance(most_common_move, RockAction):\n          return PaperAction()\n      elif isinstance(most_common_move, PaperAction):\n          return ScissorsAction()\n      else: # most_common_move is ScissorsAction\n          return RockAction()\n  else: # 1st game play randomly \n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\nOur counter-strategy:\n\nIf our most frequent action was Rock, play Scissors\nIf our most frequent action was Paper, play Rock\nIf our most frequent action was Scissors, play Paper\n\nOur best EV: +1 per game (after 1st)\nOur counter-strategy code: Exercise for the reader!",
    "crumbs": [
      "About",
      "Intro to Opponent Modeling"
    ]
  },
  {
    "objectID": "rpsom.html#general-strategies",
    "href": "rpsom.html#general-strategies",
    "title": "RPS.bot: Opponent Modeling Basics",
    "section": "General strategies",
    "text": "General strategies\nIn poker, it’s common to start by attempting to play the game theory optimal (GTO) strategy and then as you see weaknesses in opponents, to deviate accordingly. If you see your opponent folds too much, then you can bluff more.\nHere are a few possible general strategic ideas for RPS:\n\nDetect the exact opponent strategy and exploit it\n\nBuild an ensemble of opponent detectors and exploits that target each of them\n\nUse random (game theory optimal) play to either:\n\nBreakeven forever\nStart with random and then update after you learn more about your opponent\nUse it as a backup plan to switch to if your current strategy is not going well\n\nAssume by default that the opponent is playing approximately GTO and then detect deviations and play accordingly\nAttempt to predict the next move of each opponent using a general algorithm\n\n\nBreakeven unless you’re playing Rockbot\nYou could check to see if any bot is always playing Rock. If so, you always play Paper. If not, you play randomly. Then you will breakeven against everyone, but crush the Rock-only bot.\nHere’s how to execute this strategy:\ndef get_action(self, *, match_clock):\n  if self.history: # Check if the opponent has played at least once\n      # Get all of the opponent's moves\n      opponent_moves = [move[1] for move in self.history]\n      \n      # Count how many times the opponent played Rock\n      rock_count = sum(1 for move in opponent_moves if isinstance(move, RockAction))\n      \n      # If opponent has only played Rock, we play Paper\n      if rock_count == len(opponent_moves):\n          return PaperAction()\n\n  # If it's the first move or opponent hasn't only played Rock, play randomly\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n\nRandom play backup plan\nAssuming a setting with suboptimal bots, breaking even by playing randomly won’t be the most rewarding strategy, but could be valuable to consider in some situations. Here is an example of using random play in combination with another strategy.\nSuppose that you have a theory that a winning strategy is to beat the action that the opponent played two games ago.\nYou could try something like this:\n\nIf you’re winning or tying, then play to beat the action from two games ago 80% of the time and play randomly 20% of the time so that you aren’t too predictable\nIf you’re losing and down by more than 10 units (i.e. maybe this strategy is not working well), then always play randomly\n\ndef get_action(self, *, match_clock):\n\n  # If losing, play randomly\n  if self.my_profit &lt; -10:\n      return random.choice([RockAction(), PaperAction(), ScissorsAction()])\n\n  # If winning or tying, and there are at least 2 rounds of history\n  if len(self.history) &gt;= 2:\n      # 80% chance to play based on opponent's action from 2 rounds ago\n      if random.random() &lt; 0.8:\n          two_rounds_ago = self.history[-2][1]  # Opponent's action from 2 rounds ago\n          if isinstance(two_rounds_ago, RockAction):\n              return PaperAction()\n          elif isinstance(two_rounds_ago, PaperAction):\n              return ScissorsAction()\n          else:  # ScissorsAction\n              return RockAction()\n\n  # In all other cases (including 1st 2 rounds), play randomly\n  return random.choice([RockAction(), PaperAction(), ScissorsAction()])",
    "crumbs": [
      "About",
      "Intro to Opponent Modeling"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "RPS Hackathon @ Recurse: Tests and Logs",
    "section": "",
    "text": "Latest submission versus last scoring round\n\n\nLoading…\n\n\n\n\n\nNext round\n\nLoading…\n\nMatch logs\n\n\nLoading…"
  },
  {
    "objectID": "leaderboard.html",
    "href": "leaderboard.html",
    "title": "RPS Hackathon @ Recurse: Leaderboard",
    "section": "",
    "text": "Latest freeplay round\n\n\nLoading…\n\n\n\n\n\nLatest scoring round\n\n\nLoading…\n\n\n\n\n\nNext round\n\n\n\nAll scoring rounds\n\n\nLoading…"
  },
  {
    "objectID": "rpsbotchallenge.html",
    "href": "rpsbotchallenge.html",
    "title": "RPS.bot: Bot Challenge",
    "section": "",
    "text": "RPS bot challenges are coming soon!\nIn these challenges, there are always suboptimal “house bots”. Think bots that always play Rock or always play the sequence “RPSRPSRPS…”.\nThis means that playing the game theory optimal strategy will not maximize your payoff against them.\nThe challenge is to predict what they’re doing, and in cases where there are house bots and human-made bots, the challenge is to play well against the house bots without getting counterplayed by the human-made ones. And perhaps to counterplay the human-made bots that are prioritizing counterplaying the house bots!",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#rock-practice-gym",
    "href": "rpsbotchallenge.html#rock-practice-gym",
    "title": "RPS.bot: Bot Challenge",
    "section": "Rock Practice Gym",
    "text": "Rock Practice Gym\nTest your bot against a variety of our house bots in our Rock Practice Gym.",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#daily-challenge",
    "href": "rpsbotchallenge.html#daily-challenge",
    "title": "RPS.bot: Bot Challenge",
    "section": "Daily Challenge",
    "text": "Daily Challenge\nPlay 200 [how many?] games against a single RPS bot each day.\nThe bots will get roughly more difficult as the week goes on from Monday to Sunday. They’ll be all different types – our own house bots, human-made bots from our weekly challenges, LLM-generated, etc.\nYou can play against the bot with your own bot, manually, or both. We have tools to help write your own bot even if you don’t know how to code.\n[What about the timer play every second thing?] [Do daily ones also enter into the weekly?]",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#weekly-challenge",
    "href": "rpsbotchallenge.html#weekly-challenge",
    "title": "RPS.bot: Bot Challenge",
    "section": "Weekly Challenge",
    "text": "Weekly Challenge\nEnter your best bot to play against that week’s other human-made bot entrants as well as a selection of our own house bots that will vary each week. You’ll play every other entrant for 1000 games.\nEntries start each Monday and end each Sunday, with results posted the Monday after. [Maybe: Some feedback mechanism to improve and resubmit throughout week, at least against house bots?]\n\nEach week has a special rookie division for those who are entering for their first time.",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpsbotchallenge.html#hackathon",
    "href": "rpsbotchallenge.html#hackathon",
    "title": "RPS.bot: Bot Challenge",
    "section": "Hackathon",
    "text": "Hackathon\nWe periodically run virtual and in-person hackathons that we’ll announce on RPS.bot and on our mailing list/social channels.\nThe hackathons are run over a few hours with new rounds every 30 minutes. The format is similar to the weekly challenge above, where you face off against other human-made bots and our own house bots. We enter a new selection of progressively smarter house bots every round. Once a house bot enters, it stays in for subsequent rounds.\nAfter each round, you can see the entire play history against each bot and a matrix score report.\n\nYou can submit multiple times during a round, and for each submission, you’ll get a test result showing how your bot would have performed in the previous round. Doing better in the previous round is a sign of progress, but may not actually result in a better score in the current round, which will have updated participant bots and new house bots.",
    "crumbs": [
      "About",
      "RPS Bot Challenge"
    ]
  },
  {
    "objectID": "rpstheory.html",
    "href": "rpstheory.html",
    "title": "RPS.bot: Theory",
    "section": "",
    "text": "To get started, click the Login button to the left and then go to Submit Your Bot. On the bot submission page, it’s possible to submit with:\n\nCode text boxes directly on the webpage\nFile upload a single player.py Python file"
  },
  {
    "objectID": "rpstheory.html#submitting-bots",
    "href": "rpstheory.html#submitting-bots",
    "title": "RPS.bot: Theory",
    "section": "",
    "text": "To get started, click the Login button to the left and then go to Submit Your Bot. On the bot submission page, it’s possible to submit with:\n\nCode text boxes directly on the webpage\nFile upload a single player.py Python file"
  },
  {
    "objectID": "rpstheory.html#game-engine",
    "href": "rpstheory.html#game-engine",
    "title": "RPS.bot: Theory",
    "section": "Game Engine",
    "text": "Game Engine\nIf you would like to clone the repository with the game engine, you can find it at https://github.com/pokercamp/rps-engine.\nIn players/default, the player.py file is where you write your bot. We don’t recommend changing any other files.\nThe engine is in engine.py. You can use engine.py to run two bots against each other. You can use this to test your bot against itself or other bots that you create.\nThe following code will run n_games between p1_name and p2_name and output the result to the specified output_dir.\nThe generic usage is:\npython3 engine.py -p1 {p1_name} {p1_file_path} -p2 {p2_name} {p2_file_path} -o {output_dir} -n {n_games}\"\nFor example, to run a 200 game match with two bots that are named p1 and p2 with files in the players/default/ folder and outputted to the p1p2test folder, do this:\npython3 engine.py -p1 'p1' players/default/ -p2 'p2' players/default/ -o p1p2test -n 200\nThe output files are in the folder p1p2test:\n\nscores.p1.p2.txt contains the raw scores of each player\nThe p1.p2 folder contains:\n\n\ngamelog.txt: A log of all hands played\nOther log files for each player"
  }
]